{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 6\n",
    "\n",
    "Neste desafio, vamos praticar _feature engineering_, um dos processos mais importantes e trabalhosos de ML. Utilizaremos o _data set_ [Countries of the world](https://www.kaggle.com/fernandol/countries-of-the-world), que contém dados sobre os 227 países do mundo com informações sobre tamanho da população, área, imigração e setores de produção.\n",
    "\n",
    "> Obs.: Por favor, não modifique o nome das funções de resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Setup_ geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as sct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, fetch_20newsgroups\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, Binarizer, KBinsDiscretizer,\n",
    "    MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    ")\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algumas configurações para o matplotlib.\n",
    "#%matplotlib inline\n",
    "\n",
    "#from IPython.core.pylabtools import figsize\n",
    "\n",
    "\n",
    "#figsize(12, 8)\n",
    "\n",
    "#sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"Country\", \"Region\", \"Population\", \"Area\", \"Pop_density\", \"Coastline_ratio\",\n",
    "    \"Net_migration\", \"Infant_mortality\", \"GDP\", \"Literacy\", \"Phones_per_1000\",\n",
    "    \"Arable\", \"Crops\", \"Other\", \"Climate\", \"Birthrate\", \"Deathrate\", \"Agriculture\",\n",
    "    \"Industry\", \"Service\"\n",
    "]\n",
    "\n",
    "countries.columns = new_column_names\n",
    "\n",
    "countries.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observações\n",
    "\n",
    "Esse _data set_ ainda precisa de alguns ajustes iniciais. Primeiro, note que as variáveis numéricas estão usando vírgula como separador decimal e estão codificadas como strings. Corrija isso antes de continuar: transforme essas variáveis em numéricas adequadamente.\n",
    "\n",
    "Além disso, as variáveis `Country` e `Region` possuem espaços a mais no começo e no final da string. Você pode utilizar o método `str.strip()` para remover esses espaços."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia sua análise a partir daqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua análise começa aqui.\n",
    "countries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country              object\n",
       "Region               object\n",
       "Population            int64\n",
       "Area                  int64\n",
       "Pop_density          object\n",
       "Coastline_ratio      object\n",
       "Net_migration        object\n",
       "Infant_mortality     object\n",
       "GDP                 float64\n",
       "Literacy             object\n",
       "Phones_per_1000      object\n",
       "Arable               object\n",
       "Crops                object\n",
       "Other                object\n",
       "Climate              object\n",
       "Birthrate            object\n",
       "Deathrate            object\n",
       "Agriculture          object\n",
       "Industry             object\n",
       "Service              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country              0\n",
       "Region               0\n",
       "Population           0\n",
       "Area                 0\n",
       "Pop_density          0\n",
       "Coastline_ratio      0\n",
       "Net_migration        3\n",
       "Infant_mortality     3\n",
       "GDP                  1\n",
       "Literacy            18\n",
       "Phones_per_1000      4\n",
       "Arable               2\n",
       "Crops                2\n",
       "Other                2\n",
       "Climate             22\n",
       "Birthrate            3\n",
       "Deathrate            4\n",
       "Agriculture         15\n",
       "Industry            16\n",
       "Service             15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1\n",
    "\n",
    "Quais são as regiões (variável `Region`) presentes no _data set_? Retorne uma lista com as regiões únicas do _data set_ com os espaços à frente e atrás da string removidos (mas mantenha pontuação: ponto, hífen etc) e ordenadas em ordem alfabética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1():\n",
    "    # Retorne aqui o resultado da questão 1.\n",
    "    ls_city = countries['Region'].sort_values().apply(lambda x: x.strip()).unique().tolist()\n",
    "    return ls_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2\n",
    "\n",
    "Discretizando a variável `Pop_density` em 10 intervalos com `KBinsDiscretizer`, seguindo o encode `ordinal` e estratégia `quantile`, quantos países se encontram acima do 90º percentil? Responda como um único escalar inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2():\n",
    "    # Retorne aqui o resultado da questão 2.\n",
    "    \n",
    "    #trocando virgula por . na coluna pop_density\n",
    "    countries[\"Pop_density\"] = countries[\"Pop_density\"].str.replace(\",\", \".\").astype('float64')\n",
    "    \n",
    "    discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy = 'quantile')\n",
    "    \n",
    "    discretizer.fit(countries[[\"Pop_density\"]])\n",
    "    \n",
    "    score = discretizer.transform(countries[[\"Pop_density\"]])\n",
    "    \n",
    "    nacoes = np.percentile(score, 90, axis = 1, keepdims=True)\n",
    "\n",
    "    return int(np.sum(nacoes >= 9).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3\n",
    "\n",
    "Se codificarmos as variáveis `Region` e `Climate` usando _one-hot encoding_, quantos novos atributos seriam criados? Responda como um único escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3():\n",
    "    # Retorne aqui o resultado da questão 3.\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False, dtype=np.int)\n",
    "    \n",
    "    region_climate_encoder = one_hot_encoder.fit_transform(countries[[\"Region\",\"Climate\"]].fillna('NaN'))\n",
    "    \n",
    "    region_climate_encoder\n",
    "    \n",
    "    return int(region_climate_encoder.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4\n",
    "\n",
    "Aplique o seguinte _pipeline_:\n",
    "\n",
    "1. Preencha as variáveis do tipo `int64` e `float64` com suas respectivas medianas.\n",
    "2. Padronize essas variáveis.\n",
    "\n",
    "Após aplicado o _pipeline_ descrito acima aos dados (somente nas variáveis dos tipos especificados), aplique o mesmo _pipeline_ (ou `ColumnTransformer`) ao dado abaixo. Qual o valor da variável `Arable` após o _pipeline_? Responda como um único float arredondado para três casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_country = [\n",
    "    'Test Country', 'NEAR EAST', -0.19032480757326514,\n",
    "    -0.3232636124824411, -0.04421734470810142, -0.27528113360605316,\n",
    "    0.13255850810281325, -0.8054845935643491, 1.0119784924248225,\n",
    "    0.6189182532646624, 1.0074863283776458, 0.20239896852403538,\n",
    "    -0.043678728558593366, -0.13929748680369286, 1.3163604645710438,\n",
    "    -0.3699637766938669, -0.6149300604558857, -0.854369594993175,\n",
    "    0.263445277972641, 0.5712416961268142\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4():\n",
    "    \n",
    "    # Transformando a coluna Arable em float64\n",
    "\n",
    "    new_countries = countries.replace(',','.',regex=True)\n",
    "\n",
    "    new_countries['Arable'] = new_countries['Arable'].astype('float64')\n",
    "\n",
    "    # Coletando as colunas com int64 e float64\n",
    "\n",
    "    columns = list(new_countries.select_dtypes(include = \"number\").columns)\n",
    "\n",
    "    # Criando um dataset com test_country\n",
    "\n",
    "    country_t = (pd.DataFrame(test_country,index=countries.columns)).transpose()\n",
    "\n",
    "    # Criando o pipeline\n",
    "\n",
    "    pipeline = Pipeline(steps=[('median_imputer', SimpleImputer(strategy='median')),('std_sca',StandardScaler())])\n",
    "\n",
    "    # Aplicando a Pipeline no test_country\n",
    "\n",
    "    pipeline.fit(new_countries[columns])\n",
    "\n",
    "    # Retornando o valor de Arable\n",
    "\n",
    "    return(float(round(pipeline.transform(country_t[columns])[0][3],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.047"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 5\n",
    "\n",
    "Descubra o número de _outliers_ da variável `Net_migration` segundo o método do _boxplot_, ou seja, usando a lógica:\n",
    "\n",
    "$$x \\notin [Q1 - 1.5 \\times \\text{IQR}, Q3 + 1.5 \\times \\text{IQR}] \\Rightarrow x \\text{ é outlier}$$\n",
    "\n",
    "que se encontram no grupo inferior e no grupo superior.\n",
    "\n",
    "Você deveria remover da análise as observações consideradas _outliers_ segundo esse método? Responda como uma tupla de três elementos `(outliers_abaixo, outliers_acima, removeria?)` ((int, int, bool))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q5():\n",
    "    # Retorne aqui o resultado da questão 4.\n",
    "    # Trocando ',' por '.' e modificando para float.,\n",
    "\n",
    "    countries['Net_migration'] = countries['Net_migration'].replace(',','.',regex=True).astype(float)\n",
    "\n",
    "    # Calculando Q1, Q3 e IQR.,\n",
    "\n",
    "    Q3 = countries['Net_migration'].quantile(.75)\n",
    "\n",
    "    Q1 = countries['Net_migration'].quantile(.25)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculando Outliers,\n",
    "\n",
    "    outl_max = countries[countries['Net_migration'] > Q3 + 1.5*IQR]['Net_migration']\n",
    "\n",
    "    outl_min = countries[countries['Net_migration'] < Q1 - 1.5*IQR]['Net_migration'] \n",
    "\n",
    "    # Resultado,\n",
    "\n",
    "    return(outl_min.shape[0],outl_max.shape[0],False)\n",
    "\n",
    "    #Não devemos retirar os \\\"outliers\\\", pois, nesse caso, eles são o \\\"normal\\\" do dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 6\n",
    "Para as questões 6 e 7 utilize a biblioteca `fetch_20newsgroups` de datasets de test do `sklearn`\n",
    "\n",
    "Considere carregar as seguintes categorias e o dataset `newsgroups`:\n",
    "\n",
    "```\n",
    "categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "newsgroup = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "\n",
    "Aplique `CountVectorizer` ao _data set_ `newsgroups` e descubra o número de vezes que a palavra _phone_ aparece no corpus. Responda como um único escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q6():\n",
    "    # Retorne aqui o resultado da questão 6\n",
    "        \n",
    "        categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "\n",
    "        newsgroup = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "        # Vectorizando o data set\n",
    "\n",
    "        counter = CountVectorizer()\n",
    "\n",
    "        freq = counter.fit_transform(newsgroup.data)\n",
    "\n",
    "        # Recebendo o vocabulário\n",
    "\n",
    "        words = dict(counter.vocabulary_.items())\n",
    "\n",
    "        # Retornando a soma.\n",
    "\n",
    "        return(int(freq[:,words['phone']].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 7\n",
    "\n",
    "Aplique `TfidfVectorizer` ao _data set_ `newsgroups` e descubra o TF-IDF da palavra _phone_. Responda como um único escalar arredondado para três casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q7():\n",
    "    # Retorne aqui o resultado da questão 4.\n",
    "    # Criando o data set\n",
    "\n",
    "    categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "\n",
    "    newsgroup = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "    # Vectorizando o data set\n",
    "\n",
    "    Tfid = TfidfVectorizer()\n",
    "\n",
    "    freq = Tfid.fit_transform(newsgroup.data)\n",
    "\n",
    "    # Retornando o idf\n",
    "\n",
    "    return(round(float(freq[:,Tfid.vocabulary_['phone']].sum()),3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
